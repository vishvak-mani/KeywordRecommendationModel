{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/boudinfl/pke.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install yake\n",
    "#pip install rake_nltk\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install spacy\n",
    "#!{sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textract'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-35fa56cdd012>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpke\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtextract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'textract'"
     ]
    }
   ],
   "source": [
    "import pke\n",
    "from nltk.corpus import stopwords\n",
    "import textract\n",
    "import PyPDF2\n",
    "import os\n",
    "import argparse\n",
    "import re\n",
    "from tika  import parser as tika_parser\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import ssl\n",
    "import spacy\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('stopwords')\n",
    "nltk.download('universal_tagset')\n",
    "spacy.load('en')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "from rake_nltk import Rake, Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lobal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_list_stop_words = []\n",
    "db_investors_by_topic = {}\n",
    "db_keyword_by_topic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintHelp():\n",
    "    print(\"Help\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_by_tika(filepath):\n",
    "    raw = tika_parser.from_file(filepath)\n",
    "    return(raw['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_by_textract(filepath):\n",
    "    resume_extract_text = textract.process(filepath, encoding='ascii')\n",
    "    return(str(resume_extract_text.decode(\"ASCII\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_yake_pke(filename, text):\n",
    "    text = re.sub('[^a-zA-Z0-9|^\\-]', ' ', text)\n",
    "\n",
    "    # Remove words with digits\n",
    "    text = re.sub(\"\\S*\\d\\S*\", \"\", text).strip()\n",
    "\n",
    "    # Remove empty hyphens\n",
    "    text = re.sub(' - ', ' ', text)\n",
    "    extractor = pke.unsupervised.YAKE()\n",
    "    extractor.load_document(input=text,\n",
    "                            language='en',\n",
    "                            normalization=None)\n",
    "    stoplist = stopwords.words('english')\n",
    "    stoplist += custom_list_stop_words\n",
    "    extractor.candidate_selection(n=2, stoplist=stoplist)\n",
    "    window = 2\n",
    "    use_stems = True # use stems instead of words for weighting\n",
    "    extractor.candidate_weighting(window=window,\n",
    "                              stoplist=stoplist,\n",
    "                              use_stems=use_stems)\n",
    "    threshold = 0.7\n",
    "    keyphrases = extractor.get_n_best(n=20, threshold=threshold)\n",
    "    return keyphrases\n",
    "    for kw in keyphrases:\n",
    "        print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_files_from_folder(input_folder):\n",
    "    list_of_files = []\n",
    "    for dirpath, _, filenames in os.walk(input_folder):\n",
    "        for f in filenames:\n",
    "            if f.endswith(\".pdf\") or f.endswith(\".doc\") or f.endswith(\".docx\") or f.endswith(\".txt\") or f.endswith(\".csv\"):\n",
    "                list_of_files.append(os.path.abspath(os.path.join(dirpath, f)))\n",
    "    return list_of_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListOfStopWords(stopwords_dir):\n",
    "    global custom_list_stop_words\n",
    "    list_files = get_list_of_files_from_folder(stopwords_dir)\n",
    "    if len(list_files) == 0:\n",
    "        return\n",
    "    for file_path in list_files:\n",
    "        filename_w_ext = os.path.basename(file_path)\n",
    "        file1 = open(file_path, 'r')\n",
    "        lines = file1.readlines()\n",
    "        for line in lines:\n",
    "            custom_list_stop_words.append(line.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SuggestInvestors(input_folder, stopwords_dir):\n",
    "    global db_keyword_by_topic\n",
    "    global db_investors_by_topic\n",
    "    list_files = get_list_of_files_from_folder(input_folder)\n",
    "    if len(list_files) == 0:\n",
    "        print(\"No Files found in input folder \" + input_folder + \"\\n Exiting program\")\n",
    "        PrintHelp()\n",
    "        return\n",
    "    getListOfStopWords(stopwords_dir)\n",
    "    for file_path in list_files:\n",
    "        filename_w_ext = os.path.basename(file_path)\n",
    "        keywords = []\n",
    "        if filename_w_ext.endswith(\".pdf\"):\n",
    "            text = get_text_by_tika(file_path)\n",
    "        else:\n",
    "            text = get_text_by_textract(file_path)\n",
    "        keywords = extract_keywords_yake_pke(filename_w_ext, text)\n",
    "        keywords_score = {}\n",
    "        for kw in keywords:\n",
    "            keyword = str(kw[0])\n",
    "            if (keyword in db_keyword_by_topic):\n",
    "                if keyword not in keywords_score:\n",
    "                    keywords_score[db_keyword_by_topic[keyword]] = 1\n",
    "                else:\n",
    "                    keywords_score[db_keyword_by_topic[keyword]] += 1\n",
    "        reverse_sorted = sorted(keywords_score.items(), key=lambda x: (-x[1], x[0]))\n",
    "        key_industry = list(reverse_sorted[0])\n",
    "        print(\"Keywords for - \" + filename_w_ext)\n",
    "        print(keywords)\n",
    "        print(\"Key Industry : \" + key_industry[0])\n",
    "        if (key_industry[0] in db_investors_by_topic):\n",
    "            print (\"Suggested Investors :\")\n",
    "            investors = db_investors_by_topic[key_industry[0]]\n",
    "            print(investors)\n",
    "        print(\"\\n\")\n",
    "        continue\n",
    "        print(\"\\n\" + filename_w_ext)\n",
    "        for kw in keywords:\n",
    "            print(str(kw[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDB(sectors_dir, investors_dir):\n",
    "    global db_investors_by_topic\n",
    "    global db_keyword_by_topic\n",
    "    list_files_sectors = get_list_of_files_from_folder(sectors_dir)\n",
    "    if len(list_files_sectors) == 0:\n",
    "        print(\"No Files found in sector folder \" + sectors_dir + \"\\n Exiting program\")\n",
    "        PrintHelp()\n",
    "        return\n",
    "    list_files_investors = get_list_of_files_from_folder(investors_dir)\n",
    "    if len(list_files_investors) == 0:\n",
    "        print(\"No Files found in investors folder \" + investors_dir + \"\\n Exiting program\")\n",
    "        PrintHelp()\n",
    "        return\n",
    "    for file_path in list_files_sectors:\n",
    "        filename_w_ext = os.path.basename(file_path)\n",
    "        file1 = open(file_path, 'r')\n",
    "        lines = file1.readlines()\n",
    "        sector_name = os.path.splitext(filename_w_ext)\n",
    "        for line in lines:\n",
    "            db_keyword_by_topic[line.strip().lower()] = sector_name[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for file_path in list_files_sectors:\n",
    "        filename_w_ext = os.path.basename(file_path)\n",
    "        file1 = open(file_path, 'r')\n",
    "        lines = file1.readlines()\n",
    "        sector_name = os.path.splitext(filename_w_ext)[0].strip()\n",
    "        for line in lines:\n",
    "            db_keyword_by_topic[line.strip().lower()] = sector_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for file_path in list_files_investors:\n",
    "        filename_w_ext = os.path.basename(file_path)\n",
    "        file1 = open(file_path, 'r', encoding=\"utf-8\", errors='ignore')\n",
    "        lines = file1.readlines()\n",
    "        sector_name = os.path.splitext(filename_w_ext)\n",
    "        for line in lines:\n",
    "            line = line.replace('\"', '')\n",
    "            split_test =  line.split(\",\")\n",
    "            if (len(split_test) < 3):\n",
    "                continue\n",
    "            for i in range(len(split_test)):\n",
    "                if i < 2 or len(split_test[0]) == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    sector = split_test[i].lower().replace(\" \", \"\")\n",
    "                    if sector not in db_investors_by_topic:\n",
    "                        db_investors_by_topic[sector] = [split_test[0]]\n",
    "                    else:\n",
    "                        db_investors_by_topic[sector].append(split_test[0].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
